# -*- coding: utf-8 -*-
"""ch11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkN7Oioa_5YPL8l0F8WthTAcIXV2NDl-

# Machine Learning with PyTorch and Scikit-Learn  
# -- Code Examples

## Package version checks

Add folder to path in order to load from the check_packages.py script:
"""

import sys
sys.path.insert(0, '..')

"""Check recommended package versions:"""

!pip install py
from python_environment_check import check_packages


d = {
    'numpy': '1.21.2',
    'matplotlib': '3.4.3',
    'sklearn': '1.0',
}
check_packages(d)

"""# Chapter 11 - Implementing a Multi-layer Artificial Neural Network from Scratch

### Overview

- [Modeling complex functions with artificial neural networks](#Modeling-complex-functions-with-artificial-neural-networks)
  - [Single-layer neural network recap](#Single-layer-neural-network-recap)
  - [Introducing the multi-layer neural network architecture](#Introducing-the-multi-layer-neural-network-architecture)
  - [Activating a neural network via forward propagation](#Activating-a-neural-network-via-forward-propagation)
- [Classifying handwritten digits](#Classifying-handwritten-digits)
  - [Obtaining the MNIST dataset](#Obtaining-the-MNIST-dataset)
  - [Implementing a multi-layer perceptron](#Implementing-a-multi-layer-perceptron)
  - [Coding the neural network training loop](#Coding-the-neural-network-training-loop)
  - [Evaluating the neural network performance](#Evaluating-the-neural-network-performance)
- [Training an artificial neural network](#Training-an-artificial-neural-network)
  - [Computing the loss function](#Computing-the-loss-function)
  - [Developing your intuition for backpropagation](#Developing-your-intuition-for-backpropagation)
  - [Training neural networks via backpropagation](#Training-neural-networks-via-backpropagation)
- [Convergence in neural networks](#Convergence-in-neural-networks)
- [Summary](#Summary)

<br>
<br>
"""

# Commented out IPython magic to ensure Python compatibility.
from IPython.display import Image
# %matplotlib inline

"""# Modeling complex functions with artificial neural networks

...

## Single-layer neural network recap
"""

Image(filename='figures/11_01.png', width=600)

"""<br>
<br>

## Introducing the multi-layer neural network architecture
"""

Image(filename='figures/11_02.png', width=600)

Image(filename='figures/11_03.png', width=500)

"""<br>
<br>

## Activating a neural network via forward propagation

<br>
<br>

# Classifying handwritten digits

...

## Obtaining and preparing the MNIST dataset

The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:

- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)
- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)
- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)
- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)
"""

from sklearn.datasets import fetch_openml


X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
X = X.values
y = y.astype(int).values

print(X.shape)
print(y.shape)

"""Normalize to [-1, 1] range:"""

X = ((X / 255.) - .5) * 2

"""Visualize the first digit of each class:"""

import matplotlib.pyplot as plt

fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)
ax = ax.flatten()
for i in range(10):
    img = X[y == i][0].reshape(28, 28)
    ax[i].imshow(img, cmap='Greys')

ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
#plt.savefig('figures/11_4.png', dpi=300)
plt.show()

"""Visualize 25 different versions of "7":"""

fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)
ax = ax.flatten()
for i in range(25):
    img = X[y == 7][i].reshape(28, 28)
    ax[i].imshow(img, cmap='Greys')

ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
# plt.savefig('figures/11_5.png', dpi=300)
plt.show()

"""Split into training, validation, and test set:"""

from sklearn.model_selection import train_test_split


X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=10000, random_state=123, stratify=y)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)


# optional to free up some memory by deleting non-used arrays:
del X_temp, y_temp, X, y

"""<br>
<br>

## Implementing a multi-layer perceptron
"""

import numpy as np

##########################
### MODEL
##########################

def sigmoid(z):
    return 1. / (1. + np.exp(-z))


def int_to_onehot(y, num_labels):

    ary = np.zeros((y.shape[0], num_labels))
    for i, val in enumerate(y):
        ary[i, val] = 1

    return ary


class NeuralNetMLP:

    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):
        super().__init__()

        self.num_classes = num_classes

        # hidden
        rng = np.random.RandomState(random_seed)

        self.weight_h = rng.normal(
            loc=0.0, scale=0.1, size=(num_hidden, num_features))
        self.bias_h = np.zeros(num_hidden)

        # output
        self.weight_out = rng.normal(
            loc=0.0, scale=0.1, size=(num_classes, num_hidden))
        self.bias_out = np.zeros(num_classes)

    def forward(self, x):
        # Hidden layer
        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T
        # output dim: [n_examples, n_hidden]
        z_h = np.dot(x, self.weight_h.T) + self.bias_h
        a_h = sigmoid(z_h)

        # Output layer
        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T
        # output dim: [n_examples, n_classes]
        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out
        a_out = sigmoid(z_out)
        return a_h, a_out

    def backward(self, x, a_h, a_out, y):

        #########################
        ### Output layer weights
        #########################

        # onehot encoding
        y_onehot = int_to_onehot(y, self.num_classes)

        # Part 1: dLoss/dOutWeights
        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight
        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet
        ## for convenient re-use

        # input/output dim: [n_examples, n_classes]
        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]

        # input/output dim: [n_examples, n_classes]
        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative

        # output dim: [n_examples, n_classes]
        delta_out = d_loss__d_a_out * d_a_out__d_z_out # "delta (rule) placeholder"

        # gradient for output weights

        # [n_examples, n_hidden]
        d_z_out__dw_out = a_h

        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]
        # output dim: [n_classes, n_hidden]
        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)
        d_loss__db_out = np.sum(delta_out, axis=0)


        #################################
        # Part 2: dLoss/dHiddenWeights
        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight

        # [n_classes, n_hidden]
        d_z_out__a_h = self.weight_out

        # output dim: [n_examples, n_hidden]
        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)

        # [n_examples, n_hidden]
        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative

        # [n_examples, n_features]
        d_z_h__d_w_h = x

        # output dim: [n_hidden, n_features]
        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)
        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)

        return (d_loss__dw_out, d_loss__db_out,
                d_loss__d_w_h, d_loss__d_b_h)

model = NeuralNetMLP(num_features=28*28,
                     num_hidden=50,
                     num_classes=10)

"""## Coding the neural network training loop

Defining data loaders:
"""

import numpy as np

num_epochs = 50
minibatch_size = 100


def minibatch_generator(X, y, minibatch_size):
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)

    for start_idx in range(0, indices.shape[0] - minibatch_size
                           + 1, minibatch_size):
        batch_idx = indices[start_idx:start_idx + minibatch_size]

        yield X[batch_idx], y[batch_idx]


# iterate over training epochs
for i in range(num_epochs):

    # iterate over minibatches
    minibatch_gen = minibatch_generator(
        X_train, y_train, minibatch_size)

    for X_train_mini, y_train_mini in minibatch_gen:

        break

    break

print(X_train_mini.shape)
print(y_train_mini.shape)

"""Defining a function to compute the loss and accuracy"""

def mse_loss(targets, probas, num_labels=10):
    onehot_targets = int_to_onehot(targets, num_labels=num_labels)
    return np.mean((onehot_targets - probas)**2)


def accuracy(targets, predicted_labels):
    return np.mean(predicted_labels == targets)


_, probas = model.forward(X_valid)
mse = mse_loss(y_valid, probas)

predicted_labels = np.argmax(probas, axis=1)
acc = accuracy(y_valid, predicted_labels)

print(f'Initial validation MSE: {mse:.1f}')
print(f'Initial validation accuracy: {acc*100:.1f}%')

#!pip install torch torchvision scikit-learn

def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):
    mse, correct_pred, num_examples = 0., 0, 0
    minibatch_gen = minibatch_generator(X, y, minibatch_size)

    for i, (features, targets) in enumerate(minibatch_gen):

        _, probas = nnet.forward(features)
        predicted_labels = np.argmax(probas, axis=1)

        onehot_targets = int_to_onehot(targets, num_labels=num_labels)
        loss = np.mean((onehot_targets - probas)**2)
        correct_pred += (predicted_labels == targets).sum()

        num_examples += targets.shape[0]
        mse += loss

    mse = mse/(i+1)
    acc = correct_pred/num_examples
    return mse, acc

mse, acc = compute_mse_and_acc(model, X_valid, y_valid)
print(f'Initial valid MSE: {mse:.1f}')
print(f'Initial valid accuracy: {acc*100:.1f}%')

def train(model, X_train, y_train, X_valid, y_valid, num_epochs,
          learning_rate=0.1):

    epoch_loss = []
    epoch_train_acc = []
    epoch_valid_acc = []

    for e in range(num_epochs):

        # iterate over minibatches
        minibatch_gen = minibatch_generator(
            X_train, y_train, minibatch_size)

        for X_train_mini, y_train_mini in minibatch_gen:

            #### Compute outputs ####
            a_h, a_out = model.forward(X_train_mini)

            #### Compute gradients ####
            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \
                model.backward(X_train_mini, a_h, a_out, y_train_mini)

            #### Update weights ####
            model.weight_h -= learning_rate * d_loss__d_w_h
            model.bias_h -= learning_rate * d_loss__d_b_h
            model.weight_out -= learning_rate * d_loss__d_w_out
            model.bias_out -= learning_rate * d_loss__d_b_out

        #### Epoch Logging ####
        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)
        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)
        train_acc, valid_acc = train_acc*100, valid_acc*100
        epoch_train_acc.append(train_acc)
        epoch_valid_acc.append(valid_acc)
        epoch_loss.append(train_mse)
        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '
              f'| Train MSE: {train_mse:.2f} '
              f'| Train Acc: {train_acc:.2f}% '
              f'| Valid Acc: {valid_acc:.2f}%')

    return epoch_loss, epoch_train_acc, epoch_valid_acc

np.random.seed(123) # for the training set shuffling

epoch_loss, epoch_train_acc, epoch_valid_acc = train(
    model, X_train, y_train, X_valid, y_valid,
    num_epochs=50, learning_rate=0.1)

"""## Evaluating the neural network performance"""

plt.plot(range(len(epoch_loss)), epoch_loss)
plt.ylabel('Mean squared error')
plt.xlabel('Epoch')
#plt.savefig('figures/11_07.png', dpi=300)
plt.show()

plt.plot(range(len(epoch_train_acc)), epoch_train_acc,
         label='Training')
plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,
         label='Validation')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(loc='lower right')
#plt.savefig('figures/11_08.png', dpi=300)
plt.show()

test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)
print(f'Test accuracy: {test_acc*100:.2f}%')

"""Plot failure cases:"""

X_test_subset = X_test[:1000, :]
y_test_subset = y_test[:1000]

_, probas = model.forward(X_test_subset)
test_pred = np.argmax(probas, axis=1)

misclassified_images = X_test_subset[y_test_subset != test_pred][:25]
misclassified_labels = test_pred[y_test_subset != test_pred][:25]
correct_labels = y_test_subset[y_test_subset != test_pred][:25]

fig, ax = plt.subplots(nrows=5, ncols=5,
                       sharex=True, sharey=True, figsize=(8, 8))
ax = ax.flatten()
for i in range(25):
    img = misclassified_images[i].reshape(28, 28)
    ax[i].imshow(img, cmap='Greys', interpolation='nearest')
    ax[i].set_title(f'{i+1}) '
                    f'True: {correct_labels[i]}\n'
                    f' Predicted: {misclassified_labels[i]}')

ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
#plt.savefig('figures/11_09.png', dpi=300)
plt.show()

"""Added for 2 hidden layers ANN model

"""

# Step 1: Import Libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from sklearn.metrics import roc_auc_score

# Sigmoid Activation Function
def sigmoid(z):
    return 1. / (1. + np.exp(-z))

def int_to_onehot(y, num_labels):
    if len(y.shape) == 1:
        y = y.reshape(-1, 1)
    assert len(y.shape) == 2, "Input labels must be a 2D array after reshaping."

    ary = np.zeros((y.shape[0], num_labels))
    for i, val in enumerate(y.flatten()):
        ary[i, val] = 1
    return ary

# Step 2: Define Neural Network with Two Hidden Layers
class NeuralNetMLP2:

    def __init__(self, num_features, num_hidden1, num_hidden2, num_classes, random_seed=123):
        super().__init__()

        self.num_classes = num_classes

        rng = np.random.RandomState(random_seed)

        # First hidden layer
        self.weight_h1 = rng.normal(loc=0.0, scale=0.1, size=(num_hidden1, num_features))
        self.bias_h1 = np.zeros(num_hidden1)

        # Second hidden layer
        self.weight_h2 = rng.normal(loc=0.0, scale=0.1, size=(num_hidden2, num_hidden1))
        self.bias_h2 = np.zeros(num_hidden2)

        # Output layer
        self.weight_out = rng.normal(loc=0.0, scale=0.1, size=(num_classes, num_hidden2))
        self.bias_out = np.zeros(num_classes)

    def forward(self, x):
        # First hidden layer
        z_h1 = np.dot(x, self.weight_h1.T) + self.bias_h1
        a_h1 = sigmoid(z_h1)

        # Second hidden layer
        z_h2 = np.dot(a_h1, self.weight_h2.T) + self.bias_h2
        a_h2 = sigmoid(z_h2)

        # Output layer
        z_out = np.dot(a_h2, self.weight_out.T) + self.bias_out
        a_out = sigmoid(z_out)
        return a_h1, a_h2, a_out

    def backward(self, x, a_h1, a_h2, a_out, y):
        y_onehot = int_to_onehot(y, self.num_classes)

        # Output layer gradients
        d_loss__d_a_out = 2. * (a_out - y_onehot) / y.shape[0]
        d_a_out__d_z_out = a_out * (1. - a_out)
        delta_out = d_loss__d_a_out * d_a_out__d_z_out

        # Gradient w.r.t output weights and bias
        d_loss__dw_out = np.dot(delta_out.T, a_h2)  # (num_classes, num_hidden2)
        d_loss__db_out = np.sum(delta_out, axis=0)  # (num_classes,)

        # Second hidden layer gradients
        d_z_out__a_h2 = self.weight_out
        d_loss__a_h2 = np.dot(delta_out, d_z_out__a_h2)
        d_a_h2__d_z_h2 = a_h2 * (1. - a_h2)
        delta_h2 = d_loss__a_h2 * d_a_h2__d_z_h2

        # Gradient w.r.t second hidden layer weights and bias
        d_loss__dw_h2 = np.dot(delta_h2.T, a_h1)  # (num_hidden2, num_hidden1)
        d_loss__db_h2 = np.sum(delta_h2, axis=0)  # (num_hidden2,)

        # First hidden layer gradients
        d_z_h2__a_h1 = self.weight_h2
        d_loss__a_h1 = np.dot(delta_h2, d_z_h2__a_h1)
        d_a_h1__d_z_h1 = a_h1 * (1. - a_h1)
        delta_h1 = d_loss__a_h1 * d_a_h1__d_z_h1

        # Gradient w.r.t first hidden layer weights and bias
        d_loss__dw_h1 = np.dot(delta_h1.T, x)  # (num_hidden1, num_features)
        d_loss__db_h1 = np.sum(delta_h1, axis=0)  # (num_hidden1,)

        return (d_loss__dw_out, d_loss__db_out,
                d_loss__dw_h2, d_loss__db_h2,
                d_loss__dw_h1, d_loss__db_h1)

"""2 hidden"""

import numpy as np
import matplotlib.pyplot as plt  # For plotting
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer

# Load MNIST dataset
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
X = X.values / 255.0  # Normalize pixel values to [0, 1]
y = y.astype(int).values

print(f"Dataset shape: X={X.shape}, y={y.shape}")

# Split into train and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the model
model = NeuralNetMLP2(num_features=784, num_hidden1=500, num_hidden2=500, num_classes=10)

# Prediction function
def predict_proba(model, X):
    _, _, a_out = model.forward(X)
    return a_out

# Evaluate macro AUC
def calculate_macro_auc(model, X_test, y_test):
    y_proba = predict_proba(model, X_test)

    # Binarize labels for AUC calculation
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)

    # Calculate AUC for each class
    aucs = []
    for i in range(y_proba.shape[1]):
        if len(np.unique(y_test_binarized[:, i])) > 1:  # Ensure positive/negative samples exist
            auc = roc_auc_score(y_test_binarized[:, i], y_proba[:, i])
            aucs.append(auc)

    # Return the macro average AUC
    return np.mean(aucs)

# Training function with direct gradient updates
def train(model, X_train, y_train, X_test, y_test, num_epochs=50, learning_rate=0.01, batch_size=100):
    num_samples = X_train.shape[0]
    auc_scores = []  # Store AUC scores for each epoch

    for epoch in range(num_epochs):
        indices = np.arange(num_samples)
        np.random.shuffle(indices)

        for start_idx in range(0, num_samples, batch_size):
            end_idx = min(start_idx + batch_size, num_samples)
            batch_indices = indices[start_idx:end_idx]
            X_train_mini = X_train[batch_indices]
            y_train_mini = y_train[batch_indices]

            # Forward pass
            a_h1, a_h2, a_out = model.forward(X_train_mini)

            # Backward pass
            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h2, d_loss__d_b_h2, d_loss__d_w_h1, d_loss__d_b_h1 = model.backward(
                X_train_mini, a_h1, a_h2, a_out, y_train_mini
            )

            # Update weights and biases using gradients directly
            model.weight_h1 -= learning_rate * d_loss__d_w_h1
            model.bias_h1 -= learning_rate * d_loss__d_b_h1
            model.weight_h2 -= learning_rate * d_loss__d_w_h2
            model.bias_h2 -= learning_rate * d_loss__d_b_h2
            model.weight_out -= learning_rate * d_loss__d_w_out
            model.bias_out -= learning_rate * d_loss__d_b_out

        # Calculate AUC at the end of the epoch
        auc = calculate_macro_auc(model, X_test, y_test)
        auc_scores.append(auc)

        # Print progress for the epoch
        print(f"Epoch {epoch + 1}/{num_epochs}, Macro AUC: {auc:.4f}")

    # Return the AUC scores for plotting
    return auc_scores

# Train the model and capture AUC scores
auc_two_hidden_layers = train(model, X_train, y_train, X_test, y_test)

# Plot the AUC scores over epochs
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(auc_two_hidden_layers) + 1), auc_two_hidden_layers, marker='o', label='Macro AUC')
plt.xlabel('Epochs')
plt.ylabel('AUC Score')
plt.title('Macro AUC Score Over Epochs')
plt.grid()
plt.legend()
plt.show()

"""1 hidden"""

import numpy as np
import matplotlib.pyplot as plt  # For plotting
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer

# Load MNIST dataset
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
X = X.values / 255.0  # Normalize pixel values to [0, 1]
y = y.astype(int).values

print(f"Dataset shape: X={X.shape}, y={y.shape}")

# Split into train and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the model
model = NeuralNetMLP(num_features=784, num_hidden=50, num_classes=10)

# Prediction function
def predict_proba(model, X):
    _, a_out = model.forward(X)
    return a_out

# Evaluate macro AUC
def calculate_macro_auc(model, X_test, y_test):
    y_proba = predict_proba(model, X_test)

    # Binarize labels for AUC calculation
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)

    # Calculate AUC for each class
    aucs = []
    for i in range(y_proba.shape[1]):
        if len(np.unique(y_test_binarized[:, i])) > 1:  # Ensure positive/negative samples exist
            auc = roc_auc_score(y_test_binarized[:, i], y_proba[:, i])
            aucs.append(auc)

    # Return the macro average AUC
    return np.mean(aucs)

# Training function with direct gradient updates and AUC tracking
def train(model, X_train, y_train, X_test, y_test, num_epochs=50, learning_rate=0.01, batch_size=100):
    num_samples = X_train.shape[0]
    auc_scores = []  # Store AUC scores for each epoch

    for epoch in range(num_epochs):
        indices = np.arange(num_samples)
        np.random.shuffle(indices)

        for start_idx in range(0, num_samples, batch_size):
            end_idx = min(start_idx + batch_size, num_samples)
            batch_indices = indices[start_idx:end_idx]
            X_train_mini = X_train[batch_indices]
            y_train_mini = y_train[batch_indices]

            #### Compute outputs ####
            a_h, a_out = model.forward(X_train_mini)

            #### Compute gradients ####
            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \
                model.backward(X_train_mini, a_h, a_out, y_train_mini)

            #### Update weights ####
            model.weight_h -= learning_rate * d_loss__d_w_h
            model.bias_h -= learning_rate * d_loss__d_b_h
            model.weight_out -= learning_rate * d_loss__d_w_out
            model.bias_out -= learning_rate * d_loss__d_b_out

        # Calculate AUC at the end of the epoch
        auc = calculate_macro_auc(model, X_test, y_test)
        auc_scores.append(auc)

        # Print progress for the epoch
        print(f"Epoch {epoch + 1}/{num_epochs}, Macro AUC: {auc:.4f}")

    # Return the AUC scores for plotting
    return auc_scores

# Train the model and capture AUC scores
auc_one_hidden_layer = train(model, X_train, y_train, X_test, y_test)

# Plot the AUC scores over epochs
plt.figure(figsize=(8, 6))
plt.plot(range(len(auc_one_hidden_layer)), auc_one_hidden_layer, marker='o', label='Macro AUC')
plt.xlabel('Epochs')
plt.ylabel('AUC Score')
plt.title('Macro AUC Score Over Epochs')
plt.xticks(np.arange(0, 51, step=5))  # Ensure x-axis is 0 to 50 with steps of 5
plt.xlim(0, 50)  # Explicitly set the x-axis range
plt.grid()
plt.legend()
plt.show()

"""PyTorch"""

import numpy as np
import matplotlib.pyplot as plt  # For plotting
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer

# Load MNIST dataset
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
X = X.values / 255.0  # Normalize pixel values to [0, 1]
y = y.astype(int).values

print(f"Dataset shape: X={X.shape}, y={y.shape}")

# Split into train and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Define the fully connected ANN in PyTorch with two hidden layers
class FullyConnectedANN(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):
        super(FullyConnectedANN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.Sigmoid()
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.Sigmoid()
        self.fc3 = nn.Linear(hidden_size2, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x

# Initialize the model, loss function, and optimizer
input_size = 784
hidden_size1 = 500
hidden_size2 = 500
num_classes = 10
model = FullyConnectedANN(input_size, hidden_size1, hidden_size2, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD with momentum

# Prediction function
def predict_proba(model, X):
    model.eval()
    with torch.no_grad():
        outputs = model(X)
    return outputs.numpy()

# Evaluate macro AUC
def calculate_macro_auc(model, X_test, y_test):
    y_proba = predict_proba(model, X_test)

    # Binarize labels for AUC calculation
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)

    # Calculate AUC for each class
    aucs = []
    for i in range(y_proba.shape[1]):
        if len(np.unique(y_test_binarized[:, i])) > 1:  # Ensure positive/negative samples exist
            auc = roc_auc_score(y_test_binarized[:, i], y_proba[:, i])
            aucs.append(auc)

    # Return the macro average AUC
    return np.mean(aucs)

# Training function with AUC tracking
def train(model, X_train, y_train, X_test, y_test, num_epochs=50, batch_size=100):
    model.train()
    num_samples = X_train.shape[0]
    auc_scores = []  # To store AUC scores for each epoch

    for epoch in range(num_epochs):
        # Shuffle the data
        indices = np.arange(num_samples)
        np.random.shuffle(indices)

        for start_idx in range(0, num_samples, batch_size):
            end_idx = min(start_idx + batch_size, num_samples)
            batch_indices = indices[start_idx:end_idx]
            X_batch = X_train[batch_indices]
            y_batch = y_train[batch_indices]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Calculate AUC at the end of the epoch
        auc = calculate_macro_auc(model, X_test, y_test)
        auc_scores.append(auc)

        # Print progress for the epoch
        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Macro AUC: {auc:.4f}")

    # Return AUC scores for plotting
    return auc_scores

# Train the model and capture AUC scores
auc_pytorch_fullann = train(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)

# Plot the AUC scores over epochs
plt.figure(figsize=(8, 6))
plt.plot(range(len(auc_pytorch_fullann)), auc_pytorch_fullann, marker='o', label='Macro AUC')
plt.xlabel('Epochs')
plt.ylabel('AUC Score')
plt.title('Macro AUC Score Over Epochs')
plt.xticks(np.arange(0, 51, step=5))  # Ensure x-axis is 0 to 50 with steps of 5
plt.xlim(0, 50)  # Explicitly set the x-axis range
plt.grid()
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Dummy AUC data for demonstration (replace these with actual results from your training process)
epochs = np.arange(1, 51)

# Plotting the AUC scores
plt.figure(figsize=(10, 6))
plt.plot(epochs, auc_one_hidden_layer, label='One Hidden Layer', marker='o', linestyle='-', color='blue')
plt.plot(epochs, auc_two_hidden_layers, label='Two Hidden Layers', marker='s', linestyle='--', color='green')
plt.plot(epochs, auc_pytorch_fullann, label='PyTorch FullANN', marker='^', linestyle='-.', color='red')

# Add labels, title, and legend
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('AUC Score', fontsize=14)
plt.title('AUC Score Comparison Across Models', fontsize=16)
plt.legend(fontsize=12)
plt.grid(alpha=0.3)

# Show the plot
plt.show()

"""<br>
<br>

# Training an artificial neural network

...

## Computing the loss function
"""

Image(filename='figures/11_10.png', width=300)

"""<br>
<br>

## Developing your intuition for backpropagation

...

## Training neural networks via backpropagation
"""

Image(filename='./figures/11_11.png', width=400)

Image(filename='figures/11_12.png', width=500)

Image(filename='figures/11_13.png', width=500)

"""<br>
<br>

# Convergence in neural networks
"""

Image(filename='figures/11_14.png', width=500)

"""<br>
<br>

...

# Summary

...

---

Readers may ignore the next cell.
"""

! python ../.convert_notebook_to_script.py --input ch11.ipynb --output ch11.py